<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TALC:Time-Aligned Captions for Multi-Scene Text-to-Video Generation">
  <meta name="keywords" content="video,video generation,time-aligned, talc">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TACL</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

<<<<<<< HEAD
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
=======
<!--   <link rel="icon" href="./static/images/dove.png"> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
>>>>>>> 185c36cf89aea368f9cb245af555bd3f151cd24a

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
<<<<<<< HEAD
          <h1 class="title is-1 publication-title">TALC:Time-Aligned Captions for Multi-Scene Text-to-Video Generation</h1>
=======
          <h1 class="title is-size-2 publication-title is-bold">
            <span class="dove" style="vertical-align: middle">TALC</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Time-Aligned Captions for Multi-Scene Text-to-Video Generation
           
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
>>>>>>> 185c36cf89aea368f9cb245af555bd3f151cd24a
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span class="author-block">
                <a href="https://sites.google.com/view/hbansal" target="_blank">Hritik Bansal</a><sup>1</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://yonatanbitton.github.io/" target="_blank">Yonatan Bitton</a><sup>2</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/michal-yarom-767bb281/" target="_blank">Michal Yarom</a><sup>2</sup>,&nbsp;</span>  
                <span class="author-block">
                <a href="https://www.linkedin.com/in/idan-szpektor-916183/" target="_blank">Idan Szpektor<sup>*</sup></a><sup>2</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://aditya-grover.github.io/" target="_blank">Aditya Grover<sup>*</sup></a><sup>1</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang<sup>*</sup></a><sup>1</sup>,&nbsp;</span>
               
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup> University of California Los Angeles,&nbsp;&nbsp;</span>
                <span class="author-block"><sup>2</sup> Google Research&nbsp;&nbsp;</span>    
              </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2311.10111"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
              <!-- Dataset Link -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/videocon/videocon"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Dataset</span>
                  <!-- Model Link -->
                <span class="link-block">
                    <a href="https://huggingface.co/datasets/videocon/videocon"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <!-- <i class="far fa-images"></i> -->
                          <p style="font-size:18px">ðŸ¤—</p>
                          <!-- ðŸ”— -->
                      </span>
                      <span>Model</span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hritikbansal/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <!-- <div class="container is-max-desktop"> -->
  <div class="container">
    <div class="hero-body" style="display: flex; justify-content: center;">
      <!-- <h2 class="title is-3" style="text-align: center;">Summary Video</h2> -->
      <video id="teaser" autoplay controls muted loop width="80%">
        <source src="./static/images/videodirectorgpt_teaser.mp4" type="video/mp4">
      </video>

<<<<<<< HEAD
=======

<!-- <section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/main_fig.png" alt="Joint Preference Optimization" width="84%"/>
              <p> (Left) We show that the conditional preference acquisition method would require the annotators to compare two responses for an identical instruction. (Right) We show that the annotators can also assign rankings jointly over instruction-response pairs. Specifically, the annotator prefers a helpful response (e.g., Apple ... Grape) over a response that ignores the context of the instruction (e.g., wear sunscreen ... litter). Our framework thus elicits preferences that are obfuscated in the prior approach. </p>
            </div>
      </div>
>>>>>>> 185c36cf89aea368f9cb245af555bd3f151cd24a
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Recent advances in diffusion-based generative modeling have led to the development of text-to-video (T2V) models that can generate high-quality videos 
          conditioned on a text prompt. Most of these T2V models often produce single-scene video clips that depict an entity performing a particular action (e.g., â€˜a red panda
          climbing a treeâ€™). However, it is pertinent to synthesize multi-scene generated videos since they are ubiquitous in the real-world (e.g., â€˜a red panda climbing a treeâ€™ followed by â€˜the red panda sleeps on the top of the treeâ€™).
          </p>

          <p>
            To generate multi-scene videos from the pretrained T2V model, we introduce Time-Aligned Captions (TALC) framework. In particular, we introduce a simple and novel
           inductive bias to the text-conditioning mechanism in the T2V architecture that makes the model aware of the temporal alignment between the video scenes and their respective scene descriptions.
          </p>

          <p>
            As a result, we find that the T2V model can generate visually consistent (e.g., entity and background) videos that adhere to
            the multi-scene text descriptions. Further, we finetune the pretrained T2V model
            with multi-scene video-text data using the TALC framework. We observe that
            TALC-finetuned model outperforms the baseline methods by 15.5 points on the 
            overall score, average of visual consistency and text adherence, across a diverse 
            task prompts and number of generated scenes under the automatic and human evaluations.
          </p>
           
          
        </div>
      </div>
    </div>
  </div>
</section>



<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Summary</h2>

        <img src="./static/images/figure2.png" alt="Teaser" width="80%">
        <div class="content has-text-justified">
          Figure 1: <b>Illustration of our two-stage framework for long, multi-scene video generation from text.</b> In the first stage,
          we employ the LLM as a video planner to craft a video plan, which provides an overarching plot for videos with
          multiple scenes, guiding the downstream video generation process. The video plan consists of scene-level text
          descriptions, a list of the entities and background involved in each scene, frame-by-frame entity layouts (bounding boxes),
          and consistency groupings for entities and backgrounds. In the second stage, we utilize <b>Layout2Vid</b>, a grounded video
          generation module, to render videos based on the video plan generated in the first stage. This module uses the same image
          and text embeddings to represent identical entities and backgrounds from video plan, and allows for spatial control over
          entity layouts through the Guided 2D Attention in the spatial attention block.
        </div>

        <br><br>

        <h3 class="title is-4">Video Planning: Generating Video Plans with LLMs</h3>
        <div class="content has-text-justified">
          <p>
            As illustrated in the blue part of Figure 1, GPT-4 (OpenAI, 2023) acts as a
            planner and provides a detailed video plan from a single text prompt to guide the downstream video generation.
            Our video plan consists of four components: (1) multi-scene descriptions: a sentence describing each scene,
            (2) entities: names along with their 2D bounding boxes, (3) background: text description of the location of each scene,
            and (4) consistency groupings: scene indices for each entity/background indicating where they should remain visually consistent. 
          </p>
          <p>
            In the first step, we use GPT-4 to expand a single text prompt into a multi-scene video plan.
            Each scene comes with a text description, a list of entities (names and their 2D bounding boxes),
            and a background. For this step, we construct the input prompt using the task instruction, one in-context example,
            and the input text from which we aim to generate a video plan. Subsequently, we group entities and backgrounds
            that appear across different scenes using an exact match. For instance, if the 'chef' appears in scenes 1-4 and
            'oven' only appears in scene 1, we form the entity consistency groupings as {chef:[1,2,3,4], oven:[1]}.
            In the subsequent video generation stage, we use the shared representations for the same entity/background
            consistency groups to ensure they maintain temporally consistent appearances.
          </p>
          <p>
            In the second step, we expand the detailed layouts for each scene using GPT-4.
            We generate a list of bounding boxes for the entities in each frame based on the list of entities and the scene description.
            For each scene, we produce layouts for 8 frames, then linearly interpolate the bounding boxes to gather bounding box information
            for denser frames (e.g., 16 frames). We utilize the [x0 , y0 , x1 , y1 ] format for bounding boxes, where each coordinate is
            normalized to fall within the range [0,1]. For in-context examples, we present 0.05 as the minimum unit for the bounding box,
            equivalent to a 20-bin quantization over the [0,1] range. 
          </p>
        </div>


        <br><br>
        <h3 class="title is-4">Video Generation: Generating Videos from Video Plans with Layout2Vid</h3>
        
        <img src="./static/images/figure3.png" alt="Teaser" width="80%">
        <div class="content has-text-justified">
          <b>Figure 2: Overview of (a) spatio-temporal blocks within the diffusion UNet of our Layout2Vid and
          (b) Guided 2D Attention present in the spatial attention module.</b> (a) The spatio-temporal block comprises
          four modules: spatial convolution, temporal convolution, spatial attention, and temporal attention.
          In (b) Guided 2D Attention, we modulate the visual representation with layout tokens and text tokens.
          For efficient training, only the parameters of the Guided 2D Attention (indicated by
          the fire symbol, constituting 13% of total parameters) are trained using image-level annotations.
          The remaining modules in the spatio-temporal block are kept frozen.
        </div>
        <br><br>

        <div class="content has-text-justified">
          <p>
            Our Layout2Vid module enables layout-guided video generation with explicit spatial control over a list of entities.
            These entities are represented by their bounding boxes, as well as visual and text content. As depicted in Fig. 2,
            we build upon the 2D attention mechanism within the spatial attention module of the spatio-temporal blocks in the
            Diffusion UNet to create the Guided 2D Attention. The Guided 2D Attention takes two conditional inputs to modulate
            the visual latent representation: (a) layout tokens, conditioned with gated self-attention, and (b) text tokens
            that describe the current scene, conditioned with cross-attention. Note that we train the Layout2Vid module in
            a parameter and data-efficient manner by only updating the Guided 2D Attention parameters (while other parameters remain frozen)
            with image-level annotations (no video-level annotations).
          </p>
          <p>
            To preserve the identity of entities appearing across different frames and scenes,
            we use shared representations for the entities within the same consistency group.
            While previous layout-guided text-to-image generation models commonly only used the
            CLIP text embedding for layout control, we use the CLIP image embedding in addition
            to the CLIP text embedding for entity grounding.
          </p>
        </div>


      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Generated Examples</h2>
        <div class="content has-text-justified">
          <p>
            Boiler Content
<<<<<<< HEAD
=======
          </p>
          <p>
            In addition, we show generated videos from baseline and TALC.
>>>>>>> 185c36cf89aea368f9cb245af555bd3f151cd24a
          </p>
          
        </div>
        <div class="content is-centered has-text-centered">
          
          <h3>Coref-SV</h3>
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: A stuffed toy is lying on the road.<br>
              Scene 2: A person enters and picks the stuffed toy.<br>
            </div>
            <br>
            <div class="example-gifs">
<<<<<<< HEAD
              <div>
                <p><b>Baseline</b></p>
                <img src="./static/images/17_talc.gif" alt="Teaser" width="80%">
              </div>
              <div>
                <p><b>TALC(Ours)</b></p>
                <img src="./static/images/17_talc.gif" alt="Teaser" width="80%">
              </div>
            </div>
            <br>
          <p></p>
          </div>
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: A knife slices through a red chili pepper.<br>
              Scene 2: Fingers remove the seeds from the chili pepper.<br>
              Scene 3: The chili peppers are cut into small pieces.<br>
              Scene 4: An orange is peeled with a peeler.<br>
              </div>
              <br>
              <div class="example-gifs">
                  <div>
                    <p><b>Baseline</b></p>
                    <img src="./static/images/26_talc.gif" alt="Teaser" width="80%">
                  </div>
                  <div>
                    <p><b>TALC(Ours)</b></p>
                    <img src="./static/images/26_talc.gif" alt="Teaser" width="80%">
                  </div>
            </div>
            <br>
=======
                <div>
                  <p><b>Baseline</b></p>
                  <img src="./static/images/17_talc.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC(Ours)</b></p>
                  <img src="./static/images/17_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
          <p>Caption</p>
          </div>
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
            Scene 1: A knife slices through a red chili pepper.<br>
            Scene 2: Fingers remove the seeds from the chili pepper.<br>
            Scene 3: The chili peppers are cut into small pieces.<br>
            Scene 4: An orange is peeled with a peeler.<br>
            </div>
            <br>
            <div class="example-gifs">
                <div>
                  <p><b>Baseline</b></p>
                  <img src="./static/images/26_talc.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC(Ours)</b></p>
                  <img src="./static/images/26_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
>>>>>>> 185c36cf89aea368f9cb245af555bd3f151cd24a
          <p>Caption</p>
          </div>
          <hr>
          
          <h3>Human-in-the-Loop Editing</h3>
          <div class="content example">
            <br><br>
            <div class="example-gifs">
              <div>
                <p><b>Original Gif</b></p>
                <img src="./static/images/Original.gif" style="position: relative;top: 49%;transform: translateY(-50%);" alt="Teaser" width="80%">
              </div>
              <img src="./static/images/arrow_icon.jpg" style="height:50px;position: absolute;top: 50%;transform: translateY(-50%);">
              <div style="display: block;position: relative;text-align: center;">
                <p><b>Human Edit</b></p>
                <div style="font-size: 1.3em;">Make the horse smaller</div>
                <img src="./static/images/Edit1.gif" alt="Teaser" width="80%">
              </div>
            </div>
            <br><br>
            <div class="example-gifs">
              <div>
                <p><b>Original Gif</b></p>
                <img src="./static/images/Original.gif" style="position: relative;top: 49%;transform: translateY(-50%);" alt="Teaser" width="80%">
              </div>
              <img src="./static/images/arrow_icon.jpg" style="height:50px;position: absolute;top: 50%;transform: translateY(-50%);">
              <div style="display: block;position: relative;text-align: center;">
                <p><b>Human Edit</b></p>
                <div style="font-size: 1.3em;">Add "grassland" background</div>
                <img src="./static/images/Edit2.gif" alt="Teaser" width="80%">
              </div>
            </div>
            <br><br>
            <div class="example-gifs">
              <div>
                <p><b>Original Gif</b></p>
                <img src="./static/images/Original.gif" style="position: relative;top: 49%;transform: translateY(-50%);" alt="Teaser" width="80%">
              </div>
              <img src="./static/images/arrow_icon.jpg" style="height:50px;position: absolute;top: 50%;transform: translateY(-50%);">
              <div style="display: block;position: relative;text-align: center;">
                <p><b>Human Edit</b></p>
                <div style="font-size: 1.3em;">Add "night street" background</div>
                <img src="./static/images/Edit3.gif" alt="Teaser" width="80%">
              </div>
            </div>
            <br>
            <p>Video generation examples for human-in-the-loop editing.
              Users can modify the video plan (e.g., add/delete objects, change the background and entity layouts, etc.)
              to generate customized video contents.
              Given the same text prompt "A horse running", we provide visualizations with a smaller horse and
              different backgrounds (i.e., "night street" and "grassland").</p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop content">
    <h3 class="title">Limitations</h3>
    <p style="font-size: 0.8em;">
      Our VideoDirectorGPT framework is for research purposes and is not intended for commercial use (and therefore should be used with caution in real-world applications, with human supervision).
    </p>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{Lin2023VideoDirectorGPT,
        author = {Han Lin and Abhay Zala and Jaemin Cho and Mohit Bansal},
        title = {VideoDirectorGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning},
        year = {2023},
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The webpage was adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
