<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TALC:Time-Aligned Captions for Multi-Scene Text-to-Video Generation">
  <meta name="keywords" content="video,video generation,gpt,tal, time-aligned">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TALC</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <!-- <a class="navbar-item" href="https://keunhong.com">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a> -->
        <!-- @PAN TODO: consider adding links? -->
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://video-con.github.io/">
              VideoCon
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TALC:Time-Aligned Captions for <br> Multi-Scene Text-to-Video Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/hbansal" target="_blank">Hritik Bansal</a><sup>1</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://yonatanbitton.github.io/" target="_blank">Yonatan Bitton</a><sup>2</sup>,&nbsp;</span>
            <span class="author-block">
                <a href="https://www.linkedin.com/in/michal-yarom-767bb281/" target="_blank">Michal Yarom</a><sup>2</sup>,&nbsp;</span>  
            <span class="author-block">
              <a href="https://www.linkedin.com/in/idan-szpektor-916183/" target="_blank">Idan Szpektor<sup>*</sup></a><sup>2</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://aditya-grover.github.io/" target="_blank">Aditya Grover<sup>*</sup></a><sup>1</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang<sup>*</sup></a><sup>1</sup>,&nbsp;</span>
  
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> University of California Los Angeles,&nbsp;&nbsp;</span>
              <span class="author-block"><sup>2</sup> Google Research&nbsp;&nbsp;</span>    
            </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- @hritik: todo PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- @hritik: todo Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hritikbansal"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- @hritik: todo HuggingFace Link -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/videocon/videocon" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <p style="font-size:20px">&#x1F917;</p>
                </span>
                <span>Dataset</span>
              </a>
            </span>
            <!-- @hritik: todo HuggingFace Link -->
            <span class="link-block">
              <a href="https://huggingface.co/videocon/videocon-model" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <p style="font-size:20px">&#x1F917;</p>
              </span>
              <span>Model</span>
            </a>
          </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in diffusion-based generative modeling have led to the develop- ment of text-to-video (T2V) models that can generate high-quality videos condi- tioned on a 
            text prompt. Most of these T2V models often produce single-scene video clips that depict an entity performing a particular action (e.g., ‘a red panda climbing a tree’). 
            However, it is pertinent to synthesize multi-scene generated videos since they are ubiquitous in the real-world (e.g., ‘a red panda climbing a tree’ followed by ‘the red 
            panda sleeps on the top of the tree’).
          </p>

          <p>
            To generate multi-scene videos from the pretrained T2V model, we introduce Time-Aligned Captions (TALC) framework. In particular, we introduce a simple and novel inductive 
            bias to the text-conditioning mechanism in the T2V architecture that makes the model aware of the temporal alignment between the video scenes and their respective scene 
            descriptions.
          </p>

          <p>
            As a result, we find that the T2V model can generate visually consistent (e.g., entity and background) videos that adhere to the multi-scene text descriptions. Further, 
            we finetune the pretrained T2V model with multi-scene video-text data using the TALC framework. We observe that TALC-finetuned model outperforms the baseline methods by 
            15.5 points on the overall score, average of visual consistency and text adherence, across a diverse task prompts and number of generated scenes under the automatic and 
            human evaluations.
          </p>
    
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Main Figure -->
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-justified">
              <img src="static/images/main_figure.png" alt="grade-lv" width="50%"/>
              <p>Figure 1: (a) Generating video on the merged descriptions, leads to a poor text-video alignment. (b) Generating videos for the individual text descriptions and concatenate them temporally, leads to a lack of background consistency.
                (c) Our approach (TALC) enhances the scene-level text-video alignment and maintains background consistency.
             </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-justified">
              <img src="static/images/talc_videos.png" alt="grade-lv" width="50%"/>
              <p>
                Figure 1: Examples of videos generated by Time-Aligned Captions (TALC).
              </p>
            </div>
          </div>
         </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">TALC: Time-Aligned Captions for Multi-Scene T2V Generation</h3>

        <img src="./static/images/talc.png" alt="Teaser" width="80%">
        <div class="content has-text-justified">
          Figure 2: <b>The architecture of Time-Aligned Captions (TALC).</b> 
          During the generation process of the video, the initial half of the video frames are conditioned on the embeddings of the description of scene 1 (ry1 ), while the subsequent video frames are conditioned on the embeddings of the description of scene 2 (ry2 ).
        </div>

        <br><br>

        
        <div class="content has-text-justified">
          <p>
           Existing T2V generative models such as Modelscope and Lumiere are trained with large-scale datasets where each instance consists of a video and a human-written video description. 
           These videos either lack the depiction of multiple events, or the video descriptions only focus on the main event in the video. 
           As a result, the pretrained T2V generative models only synthesize single video scenes depicting individual events.
          </p>
          <p>
          <b>TALC</b> is a novel and effective framework to generate multi-scene videos from diffusion T2V generative models based on the scene descriptions. 
          TALC leverages the widely adopted text conditioning mechanism and modifies it to be aware of the alignment between text descriptions and individual video scenes. 
         </p>
          <p>
            We illustrate the framework in Figure 2. 
            TALC aims to equip the generative model with the ability to depict all the events in the multi-scene descriptions, the visual consistency is ensured by the temporal modules (attentions and convolution blocks) in the denoiser network. 
            TACL can be applied to the pretrained T2V such as Modelscope and Lumiere during inference.
          </p>
        </div>


        <br><br>
        <h3 class="title is-4">Multi-Scene Video-Text Data Generation</h3>
        
        <img src="./static/images/data_gen.png" alt="Teaser" width="80%">
        <div class="content has-text-justified">
          Figure 4: <b>The architecture of Time-Aligned Captions (TALC).</b> 
          During the generation process of the video, the initial half of the video frames are conditioned on the embeddings of the description of scene 1 (ry1 ), while the subsequent video frames are conditioned on the embeddings of the description of scene 2 (ry2 ).
           
        </div>
        <br><br>

        <div class="content has-text-justified">
          <p>
            Our TALC approach improves multi-scene video generation, but struggles with accurate text adherence due to limited training on relevant data. 
            Multi-scene video-text datasets are scarce and challenging to compile because creating detailed captions requires considerable time and resources. 
            Previous projects like ActivityNet have generated captions for specific video scenes, but these scenes often overlap or are spaced far apart, which is problematic for producing smooth transitions in multi-scene videos. 
            Consequently, there is a lack of continuous, high-quality captions needed for effective training of T2V models.
          </p>
          <p>
            We develop a comprehensive multi-scene video-text dataset that will enhance the training of our existing T2V models. 
            Using the multimodal model Gemini-Pro-Vision, we aim to create synthetic, high-quality video-text data. 
            We start with a basic video-text dataset and use the PySceneDetect library to identify different scenes within a video. 
            We then select a representative frame from each scene to capture its essence. 
            These frames, along with the overall video caption, are fed into a large multimodal model to generate coherent, scene-specific captions. 
            Using this method, illustrated in Figure 4, we generated 20k video scene-captions. The generated dataset consists of 73% multi-scene videos. 
          </p>
        </div>


      </div>
    </div>
  </div>
</section>


<!-- Experiments -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>

        <img src="./static/images/results.png" alt="Teaser" width="80%">
        <div class="content has-text-justified">
          <b>Figure 4: Automatic evaluation results for (a) ModelScope and (b) Lumiere.</b> 
          In (a), we observe that TALC-finetuned ModelScope model achieves the highest overall score, that is the average of the visual consistency and text adherence scores. 
          In (b), we find that TALC framework with the Lumiere base model outperforms merging captions and merging videos on the overall scores. We report the average performance 
          across the diverse multi-scene prompts and the number of generated scenes.
        </div>

        <br><br>
        <div class="content has-text-justified">
          <p>
            We show the efficacy of TALC in multi-scene video generation by utilizing two models: ModelScope and Lumiere.
            We show two settings, one where TALC is applied to the base model during inference and one where we finetune Modelscope on the curate multi-scene video data with TALC framework. 
          </p>
          <p>
          <b>TALC outperforms the baselines without any finetuning</b> 
          Figure 4(a) and 4(b) show that the base ModelScope and Lumiere with TALC achieves the highest overall scores in multi-scene video generation, outperforming other methods such as merging captions and merging videos.
          TALC and merging captions excel in visual consistency, whereas merging videos struggle with maintaining consistent visual elements across scenes. TALC also leads in text adherence, demonstrating its effectiveness 
          in aligning closely with scene-specific descriptions, unlike merging videos which, while high in text adherence, fail to integrate descriptions across multiple scenes effectively. 
          This indicates that TALC is particularly effective in producing coherent and textually consistent multi-scene videos.
         </p>
          <p>
            <b>Finetuning with TALC achieves the best performance.</b>
            The ModelScope T2V model was finetuned using the TALC framework and merging captions method to improve performance on multi-scene video-text data. 
            The finetuning led to TALC achieving the highest overall score, maintaining strong visual consistency while significantly enhancing text adherence. 
            Conversely, finetuning with merging captions resulted in a notable decrease in visual consistency. 
            This suggests that finetuning on multi-scene data predominantly benefits text adherence, particularly when using the TALC approach.
          </p>
        </div>


      </div>
    </div>
  </div>
</section>

<!-- Examples -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Generated Examples</h2>
        <div class="content has-text-justified">
          <p>
            We show videos generated using merging captions baseline and TALC for various multi-scene descriptions. Overall, we find that the merging captions baseline generates poor quality videos. 
            This highlights that finetuning a T2V model with multi-scene video-text data by naively merging the scene-specific descriptions in the raw text space leads to undesirable artifacts in the generated video.
          </p>
          
        </div>
        <div class="content is-centered has-text-centered">
          
          <!-- <h3>Coref-SV</h3> -->
          <!-- example -->
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: Superman is surfing on the waves.<br>
              Scene 2: The Superman falls into the water..<br>
            </div>
            <br>
            <div class="example-gifs">
                <div>
                  <p><b>Merging Captions</b></p>
                  <img src="./static/images/7_baseline.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC (Ours)</b></p>
                  <img src="./static/images/7_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
          <!-- <p>Caption.</p> -->
          </div>

          <!-- example -->
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: A stuffed toy is lying on the road.<br>
              Scene 2: A person enters and picks the stuffed toy.<br>
            </div>
            <br>
            <div class="example-gifs">
                <div>
                  <p><b>Merging Captions</b></p>
                  <img src="./static/images/17_baseline.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC (Ours)</b></p>
                  <img src="./static/images/17_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
          <!-- <p>Caption.</p> -->
          </div>

          <!-- example -->
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: A cook pours the batter into a pan.<br>
              Scene 2: The cook stirs the batter in the pan.<br>
              Scene 3: The cook puts the cake on the table.<br>
              Scene 4: The cook cuts the cake on the table.<br>
            </div>
            <br>
            <div class="example-gifs">
                <div>
                  <p><b>Merging Captions</b></p>
                  <img src="./static/images/32_baseline.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC (Ours)</b></p>
                  <img src="./static/images/32_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
          <!-- <p>Caption.</p> -->
          </div>


          <!-- example -->
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: Red panda is moving in the forest.<br>
              Scene 2: The red panda spots a treasure chest.<br>
              Scene 3: The red panda finds a map inside the treasure chest.<br>
            </div>
            <br>
            <div class="example-gifs">
                <div>
                  <p><b>Merging Captions</b></p>
                  <img src="./static/images/20_baseline.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC (Ours)</b></p>
                  <img src="./static/images/20_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
          <!-- <p>Caption.</p> -->
          </div>

          <!-- example -->
          <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: A labrador moves towards the camera.<br>
              Scene 2: The labrador moves away from the camera.<br>
            </div>
            <br>
            <div class="example-gifs">
                <div>
                  <p><b>Merging Captions</b></p>
                  <img src="./static/images/31_baseline.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC (Ours)</b></p>
                  <img src="./static/images/31_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
          <!-- <p>Caption.</p> -->
          </div>

           <!-- example -->
           <div class="content example">
            <div class="example-prompt" style="text-align: left; width: 80%;margin:auto">
              Scene 1: A koala climbs a tree.<br>
              Scene 2: The koala eats the eucalyptus leaves.<br>
              Scene 3: The koala takes a nap. <br>
            </div>
            <br>
            <div class="example-gifs">
                <div>
                  <p><b>Merging Captions</b></p>
                  <img src="./static/images/30_baseline.gif" alt="Teaser" width="80%">
                </div>
                <div>
                  <p><b>TALC (Ours)</b></p>
                  <img src="./static/images/30_talc.gif" alt="Teaser" width="80%">
                </div>
            </div>
            <br>
          <!-- <p>Caption.</p> -->
          </div>
          
        </div>

      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop content">
    <h3 class="title">Ethical Considerations</h3>
    <p style="font-size: 0.8em;">
      Our TALC framework is for research purposes and is not intended for commercial use (and therefore should be used with caution in real-world applications, with human supervision).
    </p>
  </div>
</section> -->

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{Bansal2024talc,
        author = {Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, Kai-Wei Chang},
        title = {TALC:Time-Aligned Captions for Multi-Scene Text-to-Video Generation},
        year = {2024},
}
    </code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The webpage was adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
